{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 16 - Comparison of Regression Techniques\n",
    "\n",
    "### The following topics are discussed in this notebook:\n",
    "* Simple Linear Regression (SLR), Lasso Regression, KNN Regression\n",
    "* Normalization\n",
    "* Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a dataset, with features stored in `X` and continuous labels stored in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i examples/example13.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into subsets for using in training, validation, and testing. We will use an 80/10/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000,)\n",
      "(2000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=1)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Model\n",
    "\n",
    "We will start by create a simple linear regression model using all 10 features. There are no hyperparameters to tune in SLR, so we will not need to use the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slr_mod = LinearRegression()\n",
    "slr_mod.fit(X_train, y_train)\n",
    "\n",
    "print('--- SLR Model ---')\n",
    "print('Training r2:', slr_mod.score(X_train, y_train))\n",
    "print('Testing r2: ', slr_mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(slr_mod.intercept_)\n",
    "print(slr_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (Linear Regression with Regularization)\n",
    "\n",
    "We will now use lasso regression to perform regularization and model selection. We need to use the validation set to select an appropriate value for the hyperparameter $\\alpha$.\n",
    "\n",
    "When applying the lasso regression model, it is usually a good idea to normalize your features. This can be done by setting `normalize=True` when creating an instance of the `Lasso` class. \n",
    "\n",
    "Normalization is performed by applying the transformation to $\\Large w^{(k)}_i = \\frac{x^{(k)}_i - min(x^{(k)})}{max(x^{(k)}) - min(x^{(k)})}$ to each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "alphas = np.arange(.01,5,.01)\n",
    "\n",
    "for a in alphas:\n",
    "    temp_mod = Lasso(alpha=a, normalize=True)\n",
    "    temp_mod.fit(X_train, y_train)\n",
    "    train_scores.append(temp_mod.score(X_train, y_train))\n",
    "    val_scores.append(temp_mod.score(X_val, y_val))\n",
    "    \n",
    "plt.close()\n",
    "plt.plot(alphas, train_scores, label = \"Training\")\n",
    "plt.plot(alphas, val_scores, label = \"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation curve is at its highest at or near $\\alpha = 0$, indicating that there is not a strong need for regularization. We will still consider a couple of models with small values of alpha, to try to get a sense as to which features are most relevant to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_mod = Lasso(alpha=0.13, normalize=True)\n",
    "lasso_mod.fit(X_train, y_train)\n",
    "\n",
    "print('--- Lasso Model ---')\n",
    "print('Training r2:', lasso_mod.score(X_train, y_train))\n",
    "print('Testing r2: ', lasso_mod.score(X_test, y_test))\n",
    "\n",
    "print()\n",
    "print('--- Coefficients ---')\n",
    "print(lasso_mod.intercept_)\n",
    "print(lasso_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that features 3, 6, and 7 are most relevant to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression with Selected Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso model has suggested that features 3, 6, and 7 are the ones most useful for predicting values of `y`. We will now create an SLR model using only these variables, and will compare the results with what we obtained from SLR using all 10 features. \n",
    "\n",
    "Note that removing features from the dataset, will **always** reduce the **training $r^2$** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train[:,[3,6,7]]\n",
    "X_val_sel = X_val[:,[3,6,7]]\n",
    "X_test_sel = X_test[:,[3,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slr_mod_sel = LinearRegression()\n",
    "slr_mod_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "print('--- SLR w/ Selected ---')\n",
    "print('Training r2:', slr_mod_sel.score(X_train_sel, y_train))\n",
    "print('Testing r2: ', slr_mod_sel.score(X_test_sel, y_test))\n",
    "\n",
    "print()\n",
    "print('--- Coefficients ---')\n",
    "print(slr_mod_sel.intercept_)\n",
    "print(slr_mod_sel.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When restricting our SLR to only three features, we saw a slight drop in training $r^2$ (as expected) and a slight increase in testing $r^2$. Although, these changes are not particularly significant, we are likely to prefer the model with only three features, as it is simpler and is less likely to include features that are unrelated to the label `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN on Original Data\n",
    "\n",
    "We now turn our attention to applying a KNN regression algorithm to the data. We will need to use the validation set to select an appropriate value for the hyperparameter K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "rng = np.arange(10,200,10)\n",
    "\n",
    "for K in rng:\n",
    "    temp_mod = KNeighborsRegressor(K, algorithm=\"brute\")\n",
    "    temp_mod.fit(X_train, y_train)\n",
    "    #train_scores.append(temp_mod.score(X_train, y_train))\n",
    "    val_scores.append(temp_mod.score(X_val, y_val))\n",
    "    \n",
    "plt.close()\n",
    "#plt.plot(rng, train_scores)\n",
    "plt.plot(rng, val_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model did **VERY** poorly. The $r^2$ values are negative. This means that the model not only fails to explain any of the variance in the $y$ values, it actually introduces more noise into the system. \n",
    "\n",
    "To get a sense as the why this might be, lets take a look at how the features in our dataset are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Column Minima')\n",
    "print(np.apply_along_axis(arr=X, axis=0, func1d=np.min ))\n",
    "\n",
    "print('\\n' + 'Column Maxima')\n",
    "print(np.apply_along_axis(arr=X, axis=0, func1d=np.max ))\n",
    "\n",
    "print('\\n' + 'Column Means')\n",
    "print(np.apply_along_axis(arr=X, axis=0, func1d=np.mean ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with our KNN model was that the scale of the features varied wildly from one feature to the next. We can fix this by applying normalization. We will now normalize each feature according to the formula $\\Large w^{(k)}_i = \\frac{x^{(k)}_i - min(x^{(k)})}{max(x^{(k)}) - min(x^{(k)})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Scikit-learn provides a function called `MaxAbsScaler` for performing normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution for our normalized features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Column Minima')\n",
    "print(np.apply_along_axis(arr=X_scaled, axis=0, func1d=np.min ))\n",
    "\n",
    "print('\\n' + 'Column Maxima')\n",
    "print(np.apply_along_axis(arr=X_scaled, axis=0, func1d=np.max ))\n",
    "\n",
    "print('\\n' + 'Column Means')\n",
    "print(np.apply_along_axis(arr=X_scaled, axis=0, func1d=np.mean ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create training, validation, and testing sets for our new scaled dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sX_train, sX_holdout, sy_train, sy_holdout = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\n",
    "sX_val, sX_test, sy_val, sy_test = train_test_split(sX_holdout, sy_holdout, test_size=0.5, random_state=1)\n",
    "\n",
    "print(sy_train.shape)\n",
    "print(sy_val.shape)\n",
    "print(sy_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN on Scaled Data\n",
    "\n",
    "We now return to the task of applying a KNN algorithm, this time on the scaled dataset. As before, we need to use the validation set to select an appropriate *K*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "rng = range(1,60)\n",
    "\n",
    "for K in rng:\n",
    "    #print(K)\n",
    "    temp_mod = KNeighborsRegressor(K, algorithm=\"brute\")\n",
    "    temp_mod.fit(sX_train, sy_train)\n",
    "    #train_scores.append(temp_mod.score(sX_train, sy_train))\n",
    "    val_scores.append(temp_mod.score(sX_val, y_val))\n",
    "    \n",
    "plt.close()\n",
    "#plt.plot(rng, train_scores)\n",
    "plt.plot(rng, val_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that any *K* in the range from about 8 to 20 would be good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_mod = KNeighborsRegressor(8)\n",
    "knn_mod.fit(sX_train, y_train)\n",
    "\n",
    "\n",
    "print('--- KNN Model (K=8) ---')\n",
    "print('Training r2:', knn_mod.score(sX_train, y_train))\n",
    "print('Testing r2: ', knn_mod.score(sX_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with Selected Features\n",
    "\n",
    "Our lasso model from before suggested to us that features 3, 6, and 7 were the most relevant for the task of predicting `y`. Let's try applying the KNN algorithm to a dataset that includes only these three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sX_train_sel = sX_train[:,[3,6,7]]\n",
    "sX_val_sel = sX_val[:,[3,6,7]]\n",
    "sX_test_sel = sX_test[:,[3,6,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must select a value of *K*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "rng = range(1,60)\n",
    "\n",
    "for K in rng:\n",
    "    temp_mod = KNeighborsRegressor(K, algorithm=\"brute\")\n",
    "    temp_mod.fit(sX_train_sel, y_train)\n",
    "    #train_scores.append(temp_mod.score(sX_train_sel, y_train))\n",
    "    val_scores.append(temp_mod.score(sX_val_sel, y_val))\n",
    "    \n",
    "plt.close()\n",
    "#plt.plot(rng, train_scores)\n",
    "plt.plot(rng, val_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_mod_sel = KNeighborsRegressor(30)\n",
    "knn_mod_sel.fit(sX_train_sel, y_train)\n",
    "\n",
    "print('--- KNN Model w/ Selected (K=20) ---')\n",
    "print('Training r2:', knn_mod_sel.score(sX_train_sel, y_train))\n",
    "print('Testing r2: ', knn_mod_sel.score(sX_test_sel, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "We will now consider a SLR model that includes a (very specifically selected) set of polynomial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xpoly = np.array([x4*x8, x4**4, x7, x8])\n",
    "Xpoly = Xpoly.transpose()\n",
    "print(Xpoly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xpoly_train, Xpoly_holdout, y_train, y_holdout = train_test_split(Xpoly, y, test_size=0.2, random_state=1)\n",
    "Xpoly_val, Xpoly_test, y_val, y_test = train_test_split(Xpoly_holdout, y_holdout, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_mod = LinearRegression()\n",
    "poly_mod.fit(Xpoly_train, y_train)\n",
    "\n",
    "print('--- Polynomial Model ---')\n",
    "print('Training r2:', poly_mod.score(Xpoly_train, y_train))\n",
    "print('Testing r2: ', poly_mod.score(Xpoly_test, y_test))\n",
    "\n",
    "print()\n",
    "print('--- Coefficients ---')\n",
    "print(poly_mod.intercept_)\n",
    "print(poly_mod.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
